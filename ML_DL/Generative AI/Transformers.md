## How is it different from Regular NN
In attention mechanisms, the computation of the attention weights
involves comparing each input element to the other. The attention
weights obtained by this approach are dynamic and input dependent.
In contrast, the weights of a convolutional or fully connected
layer are fixed after training.

